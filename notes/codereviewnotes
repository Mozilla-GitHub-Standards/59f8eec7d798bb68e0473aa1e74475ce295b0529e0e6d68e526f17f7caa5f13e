## Intro

As we write more code to support scientific research, we'd like to put a system in place that can be shown to reduce the number of bugs in that code, thereby upholding a high standard of quality for the underlying science - and we'd like to do so without an overwhelming investment in time or resources. What's more, we would like to further minimize the amount of time we as scientists spend writing code, by making the sharing, resue, and collaborative development of scientific code both viable and convenient. How can we design a process that speaks to all these concerns?

We can start by looking to our traditions surrounding writing manuscripts. The process of manuscript review gives us two major things: a set of customs and expectations for scholarly writing, that participants in a field can at least roughly adhere to even before the review process beings; and a formal and accountable process for reviewing a manuscript once it is written, to push it towards standards of excellence. The process of code review consists of similar parts: first, we must establish a set of basic customs that we believe will predispose our code to quality (the equivalent of good grammar and conventional structure); and second, we must formulate a process for examining code, to see if it not only meets these basic standards, but rises to a minimum level of quality, the details of which we are free to define, but must define clearly.

Which all begs the question: what customs and standards should we define for our code? What makes code 'good'? If our goal is to catch & eliminate bugs during development, we must be able to clearly define what we intend a piece of code to do, and present it in a way that is legible enough for a reader to assess whether it (likely) lives up to those standards. And if our goal is to enable collaboration on and reuse of code, we must find a way to build rapport with our collaborators, help them understand the goals of our code, and give them a simple but precise way of communicating back to us what contributions they would like to make to support those goals.

In other words, we need a procedure for clearly communicating about code, in order to help make errors more obvious, and smooth the process of working together - and this is exactly what a pattern of code review sets out to do. In what follows, we'll examine the three major steps in this pattern:

 - Defining the goals for a code project
 - Defining the anatomy of a contribution that lends itself to legible code and effective review
 - Laying out the process of actually reviewing code, in a step-by-step fashion designed to retain effectiveness while minimizing time committment.



## Desigining your project

All review processes require a standard to review against; and if our primary goal is to catch bugs and ensure our code is doing the tasks intended, we need to lay out just what those tasks are. By answering the question 'what are we trying to achieve?', we set a finite limit on what should and should not be included in our project; this is called defining a scope, and it gives us a critical frame of reference for judging new contributions. Many a coding project has failed due to 'feature creep' - the phenomenon of endlessly adding just one more interesting idea to the code, regardless of original intentions. Make no mistake: there is an infinite number of interesting ideas in the world. Trying to roll them all up into one project results in a monstrously huge codebase that is impossible to understand, maintain or use.

Once we have that high-level statement of what we're trying to achieve, we can begin to sketch out a rough plan for our code - where do we want to start from, and what are the steps we expect to have to go through to get to our goal? At this stage, we aren't trying to lay out every single implementation detail; rather, we want a roadmap that lets us do two things:
 - Separate and simplify the steps to achieving our goals. After sketching a first draft of a plan, points at which to divide a problem up often become obvious: these are extremely valuable opportunities! By taking advantage of opportunities to split a problem up, we turn a complicated analysis into a series of simpler tasks - and the smaller and simpler each of these tasks are, the more likely you'll be able to catch bugs in them as you examine and use the code. Furthermore, as tasks are split up, they tend to become more generic, and a piece of code that accomplishes a more generic goal is more likely to be able to be reused in the future. This is the practice of writing modular code: code that is built out of many small, simple pieces, that are easy to debug and easy to use.
 - Communicate our goals to our collaborators. A relatively simple, high-level roadmap is a valuable tool for helping our colleagues understand how to use and contribute to our software. Consider the following problem: a newcommer wants to participate in your project. What's the best possible understanding of the project we can give her within five minutes of her arrival? I call this a 'five minute plan' - a clear, high-level roadmap that can orient a collaborator quickly, so they can begin to make useful contributions as soon as possible.

So, our two goals at this stage are:
 - Define the scope of our project, so we can make judgements on what should and shouldn't be included; answer the question, 'what are we trying to achieve?'
 - Sketch a high-level plan of our project, so we can split it up into simple tasks and communicate the lay of the land to collaborators quickly and easily; this 'five minute plan' should be digestible by a newcommer inside of five minutes.


### Example

A bacteriophage ('phage' for short) is a virus that infects bacteria. In order to stave off these infections, bacteria keep chunks of phage genome in their own DNA, to serve as templates to help them recognize phages when they attack. An interesting study in immunology, then, would be to examine and compare repositories of phage and bacteria DNA to see where exactly these chunks of phage genome appear in bacterial DNA, so we can understand better this genetic archival process.

Sounds like a job for some code! We can immediately write down a simple goal for this project:

"Download phage and bacteria genomes from open databases, and identify the nature and position of matches to phage DNA within bacteria DNA."

By writing down a simple statement that circumscribes the scope of our project, we have done ourselves a great service: now, when an eager colleague comes along who is interested in something similar but not quite the same, and suggests we add functionality to look for instances of horizontal gene transfer between bacteria, we can easily judge that this would be out of scope, point to our goals, and say politely but resolutely, 'no'. Would code to identify horizontal gene transfer be interesting and useful? Absolutely! But the mistake we are trying to avoid is rolling up too much complication into one project. By keeping things focused and simple, maintaining and using our code remains fast and easy, and bugs remain easier to catch. That horizontal gene transfer idea is an excellent project in its own right - it deserves to stand on its own, in its own easily maintained project.

Now that we know what we're trying to accomplish, we can start to imagine some simple steps to get there. An enormously useful tool in this process is a simple flow chart. We can turn our original statement into a simple flow chart as follows:

[1]

Once we've drawn even this simple diagram, we can start to think more clearly about our project. Consider, for example, the 'compare and report' step - what does this mean? What will be involved in achieving this? Perhaps the place we're getting our data from reports it in some messy or awkward manner; we'll have to tidy it up into a form more useful for our purposes. Then, we want to search for matches between phage and bacteria DNA, and report those matches; the 'compare and report bubble' breaks down into something of the form:

[2]

Now, some more questions arise: will the cleaning process be the same for the phage and bacteria raw data? Probably we should consider splitting these up into separate cleaning processes for each type. And how exactly do we intend to do that 'search for matches' step - searching through data can be complicated, but we can simplify the process by using a database with built-in search functionality. Our flow chart becomes something like:

[3]

At this point, something very powerful has occurred; we've split the process up enough, that a major logical division has emerged:

[4]

Once that database is populated, we can carry on with our phage / bacteria DNA matching search. But we've also created a useful aretefact (the database) that can be reused for other tasks in the future - for example, we can tell that keen colleague that while we aren't going to write horizontal gene transfer detection algorithms into our project, she's welcome to use our database as a starting point to do it herself. What's more, we've created a logical division of labor in our own project; one grad student can work on getting the data downloaded, cleaned up and populated in the database, while another completely independently figures out how to search for matches and report the results. By splitting tasks up, we've made them simpler, easier to reuse, and easier to implement. And when we get down to the business of reviewing code, we've set ourselves up to answer much simpler questions; deciding if a piece of code is doing one simple bubble in our flow chart is much easier than deciding if it's contributing to the whole project in some poorly defined sense. Finally, this flow chart is a great example of a 'five minute plan' - a new collaborator can look at it, and quickly grasp the basic idea of what's going on.

### Don't Overdo It (The Plan Will Change)

Having a clear plan is hugely helpful in communicating with collaborators, keeping projects well-managed, and setting the task of code review up to be fast and easy. But don't overdo it. The realities of most software projects is that needs and goals are emergent - they become clearer as the project progresses, and this is especially true when you throw scientific discovery into the mix. Planning every last detail of a project up front is called waterfall design, and while it seems like it would be a logical extension of planning to make sure that plan is as detailed and complete as possible, it usually backfires. An exquisitly detailed plan written up-front almost never predicts all the problems and discoveries that emerge during the development process; and then to change such a detailed document to reflect these emergent details is a huge amount of work, which people rarely make time for in practice. The result is a project plan that is out of sync with the realities of the project, that ends up misleading and confusing newcomers, and misrepresenting the project as a whole.

Therefore, keep this plan simple and high-level, so that it can be changed easily (do not, however, let people bully you into changing the plan without due cause - remember, part of the point of this step is to help you judge what does and does not belong in this project; but that filter should be your deciding authority, justified and communicated by the plan, not the cumbersome nature of an overly-detailed strategy document). A good guiding principle is to answer the question: 'what is the best introduction to this project I can give a newcommer in five minutes?'

### Summary

In order to set up the process of code review and enable effective collaboration, start by wirting down answers to two questions:

 - What is the overall goal of this project? This gives you a mission statement that helps control the scope of the project, and make judgements on what to include.
 - What is the 'five-minute plan' for this project? Sketch out a plan for how this project is going to achieve its goals, possibly as a flow chart, that can be communicated to a new collaborator in under five minutes; look for opportunities to split tasks up into simple, independent chunks.




## Defining a good contribution

Now that we have a road map for our project, we're almost ready to start coding. But first, it will save us a lot of time and effort later if we set up some ground rules for what a good contribution to our project looks like. These ground rules should be designed to make contributions as easy as possible for a reviewer to understand and assess, and to eliminate simpler mistakes before the contribution is proposed in the first place; these guidelines ideally enforce a minimum standard of quality, while simultaneously minimizing the amount of time the reviewer (probably yourself) needs to spend combing through code. Think of them as the grammar of a contribution, and the set of basic customs that make something familliar and easily digestable; they are the minimum standards that people should self-check for before a contribution is made.

In what follows, I'll lay out and explain some common standards. The details may need to change to suit your project, but the overall structure typically remains the same.

### Communication

There is no substitute for a good conversation. Before they begin on a contribution, encourage collaborators to have a discussion with you (or whoever is maintaining the code) about what exactly it is that they'd like to do. Will the proposed new code contribute to the existing plan for the project, fix an existing problem, or change the course or nature of the project in some way? Will it fit with the existing work well? An up-front discussion about intentions helps to avoid misunderstandings and wasted effort down the line, and lets this whole process start from a place of clear communication and understanding.

This is where that five-minute plan you sketched out when you were preparing your project will come in handy. If this is a proposal for new code, ask the contributor exactly which step of the plan they plan on contributing to, and how they indend on achieving that; or, if their proposal seems out of scope, point to that road map as an explanation of why the proposal isn't the right fit. The simple act of getting people to articulate how their plans fit with yours will often bring to light problems and challenges before they turn into code contributions that aren't quite right.

Software collaboration platforms like GitHub provide facilities for these conversations in the form of an 'issue tracker' - use them. By asking a collaborator to open an issue (which works essentially like a forum or message board) to describe and discuss their intentions before they get started, not only can we start from a place of mututal understanding and avoid wasted effort on all sides, but a trail of documentation then exists; when the code contribution finally comes in, you'll have a clear record of the conversation around it that will help you decide if the new code indeed addresses the issue at hand.

That being said - a quick water cooler chat is far better than nothing. Tools aside, what really matters is establishing a clear mutual understanding of what a new piece of code is supposed to do.

### Submission Guidelines

The code review process is greatly expedited by standardization. By setting up some basic guidelines for submissions, we can enable contributors to self-check for basic requirements, and ensure that the code we have to review comes in an easily digestible format. Each project may need slightly different submission guidelines to suit its needs, but here are a set of common ones to start from; whatever list you come up with, put it in a very obvious and prominent place in your repository - probably in the README file at the top level of the project, under a clear heading of 'Submission Guidelines'.

 - Maximum submission length of 400 lines. It's impossible to review thousands of lines of code in a sitting; [this study](http://smartbear.com/smartbear/media/pdfs/wp-cc-11-best-practices-of-peer-code-review.pdf) recommends no more than 400 lines at once. Going beyond this reduces the amount of bugs we can successfully detect, while making the amount of time needed for code review blow up. If more changes than this are required, they should almost certainly be in separate contributions.
 - Maximum function length of 50 lines. The shorter and simpler a function is, the easier it is for the reviewer to confirm that it in fact does what it's supposed to; long, complicated functions are difficult to assess, and rarely lend themselves to reuse down the line.
 - Code must be automatically mergable. Version control systems will report if a new contribution can be automatically merged into the existing code, or if a conflict exists. Make it squarely the responsibility of the conrtributor to resolve all conflicts before submission, so you aren't overwhelmed with fitting things together yourself.
 - All tests must pass. One of the other benefits of insisting on small, modular functions, is that they lend themselves better to writing automated unit tests to ensure their helthy functioning. If your project contains a test suite, insist that all tests must still pass after the inclusion of a new piece of code, as an automatic cross-check to ensure nothing has been broken by the changes.
 - All functions must be tested. Similarly to the above, if a contribution creates a new function, insist that it have at least one or two tests in the test suite that check that it is doing what it's supposed to. By requiring contributors to write tests as they go along, the effort of building up a test suite remains managable and distributed.
 - All functions must be documented. Every function should be accompanied by a comment indicating, at a minimum, what the function is supposed to do, what its inputs are, and what it will return. Again - insisting contributors write these as they go along prevents the task of generating documentation from becomming overwhelmingly time consuming.
 - Respect the style guide. Most programming languages have an agreed upon guide for programming style, designed to make code as readable and standardized as possible. What's more, free utilities exist that will automatically check for compliance with these rules (see [pep8](https://pypi.python.org/pypi/pep8) for Python or [jslint](https://www.npmjs.com/package/jslint) for JavaScript, for example). Indicate to your contributors what style guide and checking tool you are using, and demand they submit code that is strictly compliant to these standards.
 - Indicate what issue this contribution addresses. As discussed above, much sadness is avoided by talking about plans for code contribution beforehand. Ask your contributors to link to the thread in the issue tracker that led to this contribution, so that you can quickly see what this contribution is trying to address.
 - Summarize and annotate changes. Ask your contributors to write up a very brief, point form description of how their contribution goes about achieving its intended task. By asking your contributors to articulate the strategy behind their work, not only do you get an easily-understood description of the contribution, but there will be less bugs on average! By thinking about what they've done enough to describe it, contributors will often catch their own mistakes before submitting them.
 - Cut and paste this checklist into the submission description, and indicate compliance with each point. By asking your contributors to copy your submission guidelines into the discussion of their submission, you get an immediate indication of whether they read the instructions; furthermore, asking people to indicate compliance with each point discourages contributors from ignoring any of the guidelines you've laid out.

If a contribution fails to respect any of the submission guidelines you lay out, look no further - you are well within your rights to politely but summarily insist these basic requirements are met before the contribution will be considered further.










 - Reviewing a contribution
